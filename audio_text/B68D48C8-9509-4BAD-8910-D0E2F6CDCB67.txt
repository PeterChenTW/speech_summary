What's your value computation rate? Second step is, do you have your data readily available? Bring the data. Make it ready. Then start analyzing that data. It's great. It's 70% of the job. The value that you can generate on the third step is maybe 10%. Everyone can do that. No machine can do that. Where the value comes from is actually when you're assigning new subject matter experts onto the role and say, oh, this correlation means that. That's exactly the value. And that's exactly the part where we have currently no standardization. But I am now saying this, which is one of my goals, to use generic AI capabilities with a little automation and scientific justification. That means that every correlation studied on to understand the parameters that are correlating with our role as parameters must generate AI that's one goal that we actually have.

standardization. Today, I will be talking about the one that is proposition two, data pipeline and data preparation, three, data models and models, how many of them, the fourth one which is very important in improving quality. So, how do you set the value proposition? Standardization is key and there are three things that we will take with you today are shown here. By using data analysis, that is actually a part of my data sharing, you could set the new quality baselines for the new technologies. And also, you could improve your performance or yield and I will tell you how. The last one is whatever you learn from the currently running apps, you can scale it to the next technological

before it goes to custom. From the quality perspective, there are two aspects. One of them is, if you know, on the right-hand side up here, if you know what raw material state that goes into your system, you can predict your output. You know your product well. This is what we are doing. We are predicting the quality of the material before being produced. And the last one, mutual metrology was mentioned also in the previous presentation. That helps us give some really high-demanding metrology steps by using predictive models. You can actually predict what you measure. You don't even get a physical sample. It's all about cost. All right. So let's come to the conclusion, actually. The motto of the EpiCon Taiwan is here is pre-spine elevation and pre-powered sustainability. And we do.

We need to take a collaboration with retail and consulates. And there are two things that are very key for this. You want to collaborate with data, drive the cultural change for the people. Everything starts with people, not technology. So help us drive the change. Help us to explain to your teams that sharing data is not actually something that they have to fear. They should be afraid of it. And that can only help them collaborate better. The second thing is that, of course, technology involves people. Use the right technology. If you want to create impact and scale, use platforms such as Atelier that we are using, also a platform, where you can collaborate in a secure way with your partners and create benefits together for the industry. And with that, actually, I am closing my presentation with one thought that resonates a lot with me.

from Andrew Stebbins, a famous statistician and engineer. In God we trust, all others must bring negative. So please bring your negatives so that we can rely on them and accept them in your process. Thank you. Our work means you will inspire every customer to provide data without including their confidential information. So, actually, we are currently working with all leading edge customers. So we don't negotiate anymore. They have seen the value. We have some use cases from R&D, from manufacturing space. We just show them that we normalize and send them the data. As I mentioned before, we don't exactly say that...

This is the encrypted gene. This, we just say, this is parameter A, and this is between 2 and 1. And that exactly protects your IP. So there is no information that is shared with the customer. That's the way that we negotiate that. Okay, thank you. A big hand to Dr. Fatou for his speech. Thank you. And then I would like to introduce...

or from different parts of your process. So, device manufacturers, you can just focus on the expertise that you're involved in. Actually, the device manufacturers can get benefit by reducing the expertise and by reducing the variations as opposed to reducing the quality. And if it comes to yield, we are identifying parameters that are beyond the certificate of analysis that helps to optimize the process further. And the last one I mentioned, imagine that you have a product that is coming from your supplier telling that a dispatch that you are having a kind of abnormality and you have an incoming quality of your system that is excellent before you send it to your process, before you start reproducing scratch. That's the value you can actually use in your environment. The last one is...

If you know what's happening right now, you can maybe help, and the new technology knows what matters most. You can try to accelerate your ramp-up by using the knowledge from this kind of studies. So, from the materials supply perspective, we have also similar values of competition. That is, the first one, quality, understanding of variations and reuse. We don't want that, right? The second thing is that you improve your process performance. I know, with 60,000 aircrafts, I am also sacrificing 60,000 aircrafts. It has been tremendously used in the industry. But now, we can use different methods, which are data-driven. We can go beyond statistical analysis. We can improve the yield and performance more. So, we are working on that.

The last one is also very important. If you have a very good manufacturing capability, you can actually accelerate that public identation. You know what matters for variations and you can show it with data and pay your customer. Don't worry about that. I have a big portal that's not going to impact your quality. Just continue with that. You can implement and change. These are the key benefits. The second step is how to prepare your data and pipeline. We have invested intensively to create an ontology on our data in the system. We get the data from ERP systems, lathes, process control systems, and we have been building this data lathe where we define what we look for. It gives us speed. It helps us to set the standards based on the data analysis. And also it helps us to understand statistically whether or not our data is important.

Well, here is the highlight of the presentation from my side. You will see now a video that shows actually how we analyze data internally and also externally with customers. In the second format, we present a similar approach with Micron, because we are collaborating with them. That's an obvious theoretical information. But here you will see how we do it also internally. So what we did actually, we used more than 200 different models to combine our data, to look at customers' data or our own users' data, to understand what are the correlations that are coming from my best-bet variations that I don't see now, because I'm only focusing on self-degraded analysis. When I did similar analysis five years ago, in two, minimum six to eight months, I justified a hypothesis that was driven by data analysis.

Here, as you can see, we can do it within two to three weeks, and multiple IP addresses can be checked. And even now, we are talking about speed. You will see in two-minute videos, all this end-to-end data analytics, discover new parameters, can happen within two minutes. Let's go to the video, and let me explain to you what this platform does for us. Actually, it starts with showing your correlations. You will have some target parameters, and some X-parameters, that are coming from your callback or your own process. And you run the correlation, and you see which target parameters have strong correlation based on machine learning models. Of course, everyone wants to have more than 80% R-squared value, that's a good prediction. Yes, but then what happens afterwards, right? You get your data. We have in our ecosystem our data, our supplier's data, and sub-supplier's data, that is really accurate.

And once you run your correlations, then what you see here is the feature importance. That means what are the parameters that are impacting or that are making a high impact on the target variety. You can immediately select two of the target varieties here. And people have the automated data analysis can show you what are the important parameters for that target variety. This is super handy. I'm really so glad to see that it helps with this speed. And what happens afterwards, you want to understand, OK, tell me some more parameters, like some parental parameters. And tell me what are the parameters correlating with the target variety. You can check that. You want to see more than five, you can just increase the number of features. You want to check if you can see more than five parameters. So this is really very powerful. I don't know how many of you have been working in similar.

trying to understand what's happening in the system. This is really super fascinating and super fast. And what happens at the end? Of course, these are just correlations, right? They don't have scientific justification. It's exactly where you need to bring your teams across your organization, your quality teams, engineering teams, manufacturing teams, R&D teams, that they can see what is happening exactly in those correlations. Whether or not they have scientific meaning. So this is what we are doing at Merck Electronics together with our semi-material organizations. And if you think about scaling this further, there are actually three areas that you can scale. You can scale per product and more and more parameters to see what's happening. That's one. The second one, you can scale multiple products or processes.

There could be also different paths, if you produce the same material, a different path. The third one is, we can scale across the ecosystem so that more and more parties can collaborate together. The key point here is, all these things are happening in one ecosystem. That helps us to understand the quality better, to provide better materials for our customers. But what you can do internally, which is actually the connection to the smart manufacturing, is that, if you look at your process on the left-hand side, you can actually analyze your process data much more intensively and understand some similarities between different batches that you produce. That can help you to optimize your own process. Or, you can use domain statistical process control charts or outright statistical process control charts to understand the variations between your manufacturing.

