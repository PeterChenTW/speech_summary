I would say currently the generative AI, number one, Microsoft, to share with us how to leverage the best practice for this generative AI, and with number one in the hardware, number two in software, and of course we will have semiconductors, tycoons from UMC to share with all of the best practice to utilize the AI to be the future smart manufacturing. So let's move to the next page. Okay, because we will have quite plenty of the audience, the time is very limited. I will encourage all of you to scan these barcodes because it will allow you to raise your questions. And later on, the speaker has a limited time.

tetapi jika kita tidak mempunyai cukup masa untuk menjawab pertanyaan, semua pertanyaan akan dikonsolidasi dan kami akan menghantar kepada pembicara untuk jawapan kemudiannya. Jadi, saya rasa tanpa lebih lanjut, saya rasa anda semua benar-benar menantikan data berharga dari forum dan pembicaraan ini. Jadi, saya akan mula memperkenalkan pembicara pertama. Adakah kita mempunyai laman perkenalkan Dr. Andrews? Baiklah, saya akan memperkenalkan Dr. Andrews. Dia berasal dari Almeria dan dia telah berada di

So, again, it's a timeline, but what we show here is the increase of the code, accumulation of the code from CAE. CAE stands for Computer Aided Engineering. So, this starts from the triangle one, the right-hand side. The GPU accelerates the data. The data here, for example, the EDA data, the CAE data. The CAE data, again, because of the more and more complex of the product design, more and more complex of the chip design, transistor becomes more. So, we need more computing power to accelerate the overall design process, engineering process, manufacturing process. So, you can see, for decades ago, CPU dominated all the CAE code. Before, when you need more computing power, just buy more CPU. But, because of this exponential growth, naturally, that we found out that most of the customers...

come to ISV, for example, ANSYS, Synopsys, Cadence. Hey guys, we need more computing power and we don't have that much space. As you know, we have ESG, we need more, we have limited resource, we have limited space. So what are some alternative ways to accelerate the overall computing? So naturally, it comes out that parallel computing is one way to solve these compressive problems. So as you can see, the CA code accumulates. Again, for CPU code, it still becomes more and more, but GPU code accumulation grows exponentially compared to CPU nodes. And one of the examples is from Synopsys. They had a really, they call it next generation SPICE. It's a circuit simulation algorithm, but before it's all dominated by CPU. But customers will come to Synopsys and say, hey,

How do I accelerate my SPICE build? Because I need to keep up with the title market cycle. I need to tap out my chip in a small limited amount of time. Customer cannot wait. So we see that Snapdisk uses a video GPU to accelerate the overall circuit simulation cycle for analog, for signal integrity, for radio frequency. And with GPU acceleration, we can see 30 times performance improvement compared to traditional CPU. This is our Synopsys. And not just for simulation. So this is more of a simulation workload. So simulation workload can be accelerated by GPU, that's for sure. It's not GPU friendly. So actually NVIDIA and Synopsys put a lot of resources to pull in the code from traditional CPU workload to GPU workload. So CPU and GPU still work together, but you need to create some smart way.

to work together between CPU and GPU, you need to put a CPU memory to GPU memory and to initialize the kernel from GPU and start the overall process. It's not a traditional CPU code. So actually, both of our two companies spend a lot of time, a lot of resources to porting the code to GPU, but the result is really significant. And actually, the driving force is from customers, like IC design company, because they need to get up the pace, they cannot wait, that's tied to market, tied to customer, this kind of a cycle is still the same. And another example is from Canon. So not just from simulation, this kind of analytics workload, this is just visualization. So again, because of the complexity of the product design, this is just a kind of a layout visualization tool. So on the left-hand side here is a table that shows the difference.

between CPU and GPU. And not just CPU, GPU. On top of GPU, we have a lot of software to accelerate the overall visualization process. Again, under the hood, is the CPU, GPU, overall architecture, and also PCIe. But we need to come up with some software to make sure that when this kind of memory comes to CPU, really, for example, use some kind of memory, memory footprint methodology to move the GPU computing more efficiently. So before, for example, just a RAM fit or show all layer, you need to take some, for example, three seconds, four seconds. And this is just one time. You can think about, I'm not sure how many of you are layout designers. When you use a layout tool, you kind of need this kind of interactive process. So three seconds is really a long time. Just a RAM fit, you need to stay focused.

For example, wait until three seconds to see a result. When you see a result, you might find, okay, this is some kind of a spot need to improve, and you do some modification, and you do some simulation. When you get back to the result, you need to kind of read the layout, and still need some kind of a lot of three seconds, and overall, you're gonna decrease your productivity. So after GPU acceleration, we can see much more improvement, and which can make this kind of interactive process more efficient, and not just for performance improvement, we can see the graph results. It become more hydro-efficient, and it's actually, I mean, improved by the software. We call it GPU rendering. And the other one come from TSMC. I think three years ago, we had a project to work with TSMC, and this is for lithography, for maximum synthesis. So, even overall.

After the lithography process, you need to have a process to inspect the mask. So you can see on the right hand side here, and by the way, on the left hand side is the GDC talk that you can find from the internet. So GDC stands for GPU Technology Conference. It's an online conference. You can put all the video talks shared by our customers to the web. If you want more detail, you can check on our GDC site. But back to our result, you can see on the right hand side here, again, all the different simulation performance improved compared to traditional CPU workload. Some of them might even to 200x performance improved. So not just for simulation, visualization, or the overall mask inspection process need to accelerate by GPU. Again, if you remember the previous slide, we have a transistor.

increased by an exponential, the computing power increased also by an exponential. So like material design software, VASP and LEMS, if you are in the very beginning of the semiconductor flow, you need to leverage this kind of simulation tool to simulate the materials, make sure the overall design match what you need, match customer's expect. So material design also have a talk to share how they use GPU to accelerate LEMS and VASP, this kind of molecular simulation software for semiconductor materials. And for macro world, ANSYS, for example, 3DIC, this kind of simulation for fluid, for packaging, you also leverage a lot of ANSYS software to simulate the overall design. So again, this talk from GDC that ANSYS engineers share how they use GPU to increase...

improve the performance by multi-GPU and multi-Node. So, X is for not just single node, but multi-Node. So, multi-Node actually become more complex because they need to bring more, for example, a mini-band 200 gig switch, overall topology need to consider, but not from ANSYS driving this, it's from customer. Customer need to accelerate the simulation process to keep up the pace, that's the idea. So, I just show you some of example from our triangle on the right hand side, GPU keep accelerate the data, EVA, CAE, simulation, visualization in semiconductor world. So, if we back to this triangle, GPU accelerate data, I think I provide so many evidence to you, and the key message or the lesson learned from us is that it's not drive by the media, the media just a computing platform. It's not driving by ISV, for example.

Synopsys. It's not driving by this ISV, software vendors. It's actually driving by all of you. You are the expert. And you are the expert manufacturer of the chip. You are the expert to design the chip. But you need to keep up the pace, keep up the overall tech market. So, all of this is just happening in these 2 to 3 years. But this is right inside. And I want to show you today another key information that NVIDIA is not just GPU. GPU is the key, but today we have CPU. We have DPU. So, DPU stands for Data Processing Unit. I think 2 years ago or 3 years ago, NVIDIA acquired another company called MeraLux. It became our networking department. So, the main product from MeraLux is 3 things. One is the NIC, Network Interface Card. The other one, InfiniBand. And the third one is the Switch. So, all these 3 things.

di kawasan sains data untuk pelajaran mesin AI dan walaupun sekarang, dia berada di Foscom dan dia telah melakukan analisis data ini dan bagaimana untuk mengintegrasi kekuatan AI ke dalam lingkungan pembuatan sebenar dan apa yang saya ingin menyampaikan kepada Dr. Andrew bolehkah kita pergi ke lantai pertama untuk Dr. Andrew? topik ini jadi tanpa laluan, saya akan menyampaikan kepada Dr. Andrew untuk memulakan ucapan ini terima kasih selamat menikmati

components is the key part to make sure that if you want to have a computing power across different nodes, you need these three key parts. You need a KNICK to communicate to other computer. You need an InfiniBand to transfer the data to different nodes. You need a switch to kind of orchestrate all the computing power. So NVIDIA today is a platform company. We have GPU that everyone should be familiar with. We have data processing unit. Actually, the easier way to think about it is that we put an ARM chip within KNICK core, which means that KNICK core can also take care by themselves. Before DPU or before smart KNICK, everything needs to go through CPU. If you want to transfer data from one node to one node, you need to communicate between different CPU nodes. And this kind of communication actually will get a lot of overhead and the performance will drop.

So DPU or GPU, in some way, the idea is to offload the computing power from CPU. CPU can still be there, but, for example, if you need parallel computing, if you need deep learning processing, if you need very complex simulation, let's use GPU. But if you need to transport data across different nodes, let's use DPU. So DPU is still there, because you still need to take care about the overall, for example, the operation system, some housekeeping tasks within the machine. But NVIDIA today is a platform company, we have free chips. And the other thing that I'm really excited about is our chip called Grasshopper chip. This is a module, we put a CPU and GPU together. Like I shared earlier from ANSYS.

For example, Kedis, they have so many different simulation codes. And most of them are actually bounded by CPU, which means all the legacy codes are run on CPU. Remember I showed you the example that we worked with, for example, Synaptics for PySim. We put a lot of resources, porting the code from CPU to GPU. It's actually needed. Somehow you need to architect the overall flow, the overall algorithms. And the reason for that is that you need to remember that CPU and GPU still work together. They cannot work independently. GPU still relies on CPU to count all the data. So usually the best way to write a CUDA code is to try to...

move out the, for example, data processing, data memory copy code. So with Grace Harper, we put two chips into a single module, which means the data communication between CPU and GPU no longer being limited. We connect it by chip-to-chip NVLink in one chip. So every CPU, before bounded by CPU node, can be assigned by Grace Harper. And NVIDIA not just find hardware, we also have a lot of software. And one thing I want to show you today is Articus. I will not go into detail today, but the idea is to let you know NVIDIA actually got a lot of SDK. Today we got more than 400 SDK. And the history is that, for example, seven years ago when I joined NVIDIA, we worked on so many different industries. And the reason we work with industry is that we need to...

learn from customers like TSMC, UMC, many customers what kind of workload you need to accelerate from GPU. We work with customers, we learn from customers, and once we learn some, okay, we need to come up with some abstract SDK to accelerate and to help all the other industries. Then we will generate or create, develop this kind of SDK. So today we have more than 400 SDK. We have Clara Genomics for sequencing and acceleration. We have Clara Medical Image for MR, CT, X-ray. We have Atropis for smart cities. We have Triumph for autonomous cars. So we have so many different SDK to accelerate the overall workflow for domain-specific applications. And for Artiverse, it's quite unique. This is a platform that we shared to the world, I think, two years ago. The idea is that...

There are so many 3D CAD data out there, so many different formats. And this is one of the examples that CAE and CAD engineers need to work together because they need simulation results. They also need to post-process it to visualize the overall outcome. So on the left-hand side here, you can see, for example, ANSYS, R-Type, Sequence, Autodesk. So again, Anibus is not trying to replace all this software. In another way, we try to provide a platform architecture. They can connect these third-party tools, like ANSYS, Sequence, R-Type, or some open-source tools. So to let customers, enterprise customers, to develop or to architect a customized 3D platform by themselves. For example, and before, it's just different information silos. Every expert just work within their workstation, work with their laptop, but then I cannot show you.

very efficient. On our universe, for example, on the left-hand side here, from all kinds of data, you can also include your IoT data, whatever data you need, but specifically for 3D data, from geometry, variance, simulation, materials, when you put, get, I mean, integrate all this data, and on the middle here is all kinds of different stakeholders that can involve overall product design process. And before, everyone just worked independently. It's an information silo, it's a system silo, so the data sharing is not that easy. But with the Omniverse, you can create your customized 3D platform. From there, these stakeholders, like concept designer, or visualization designer, or surface modeler, or CAE specialist, need to simulate the results. But everyone can work together and put all the data into single one-pan display and share across different

For example, if you have OVC in different countries, you can also leverage this kind of architecture to collaborate together. This is the R-Universe. So I will not go into too much detail, but let's back to this triangle again. So again, NVIDIA is a platform company. We provide hardware, but also software. And we see the platform accelerate the data. We also see how our platform enables deep learning science. And luckily, because of these three ingredients work together, we see another new breakthrough. Algorithms come out, it's called Generative AI, but actually it's a transformer-based architecture. So this kind of algorithm is actually still a deep learning algorithm, but it's a transformer. It leverages attention layer. And the most important thing is that it's very GPU-friendly.

which means that developers can keep increasing the model, keep consuming more data, keep buying more GPU to see what kind of result that we can have. So luckily, everyone just kind of practice in this way. So we see generative AI actually keep a lot of momentum here. We see very giant company including Microsoft, OpenAI, Meta, Google, or enterprise startups. Nowadays, startups leverage generative AI to initialize so many unique opportunities in a field. And generative AI, if you see in this graph here, it's actually just a transformer bottom, but the most important thing is very GPU-friendly. And the other very smart thing is that it's kind of a supervised learning process. It's still supervised, but it leverages

the, for example, larger amount of data on the internet and just, for example, move some of the text out and just let the model try to predict. So actually, naturally, you don't have to label the data. So let's do this. One, GPU-friendly. Because of, before genetic AI, before transformer, when people deal with sequence-to-sequence model, they would use RNN model, they would use LSTM. These two kinds of model are not GPU-friendly, which means that people cannot, developer cannot put more model complexity within LSTM, otherwise it cannot be estimated well. But because of transformer and GPU-friendly, developer can keep increasing the model size. And you might wonder why they keep increasing the model size. Because they see the scale law. The paper you can check is the law of scale. When you increase the data, increase the model, definitely you need to increase the GPU size or the number of GPUs.

Okay, good morning everyone and really appreciate the introduction from Jonathan and really thank you so much for the invitation from Scott and James and Mark, this is my pleasure, my honor to be here and we have 40 minutes so, but we got a lot to share to everyone and as Jonathan introduced in the beginning, NVIDIA is a platform company, but today we gonna share a little bit about software that is something that NVIDIA also spend a lot of time to develop and especially today's topic is generative AI, they gonna show you how everything works and how NVIDIA is a full-stack company to support our customers like enterprise like TSMC, UMC to create.

But when you increase all of them, you can see the accuracy will break through. But, so, Transformer is a very GPU friendly. And Transformer, the algorithm underlined here is, naturally, it's still supervised learning, but it learns from the data. Just load up some of the statements, load up some of the words, and then the algorithm tries to predict. So naturally, you don't have to label. Because label process, annotation process, is actually the most important challenge across the overall AI process, pipeline. So because it's too natural, naturally, and net-generated AI becomes really breakthrough. And after, I think, 2017, attention, all you need is Google's paper for translation. All the models become Transformer-based models for video, for image. And here are some of the examples from Genomics.

This is just some kind of language that people don't really understand. But transformer model also have the potential to realize the understructure of the DNA sequence on the OS's protein. So again, back to, I mean, if model can learn from structure, which means it can also learn from the materials. So a lot of people use generative AI for all kinds of modality, not just for natural language. For video, for image, for protein structure, for materials. Pharmaceutical company nowadays, they leverage generative AI to generate catalytic molecule to increase the overall drug discovery process. So it's really overwhelming across many different industries. But if we go back to generative AI or the transformer base, you can see it still falls into the artificial intelligence overall operators.

And I was somehow in this machine learning era, I mean, when I was in PhD, I remember there's not that much data out there, it's just the data at the bottom is like few hundred, few thousand. So, at that time we have support vector machine, we have decision tree, we have class tree, KLN, all kinds of machine learning algorithms, but it's just, for example, not existing in enterprise. Enterprise doesn't use that much machine learning, it's like 15 years ago, 20 years ago, that time when I graduated, I saw the proposal from that student, proposed neural network, but it's just three layers, few neural blocks. It's really small model, but because of deep learning, people will start to increase the model size. But deep learning also create an opportunity, because algorithms become an approximator. It's a supervised learning. What you need to provide is just input and output.

The companies can learn by themselves, it's called software-app-write software. Write a model, like what we did for Go-Base process. But the software learns from itself, learns from data. It's a data-driven approach. So, but after deep learning, we kind of move to another paradigm. It's a paradigm shift. It's a... Generating AI is actually what we call foundation model. So the more data you provide, the more complex model you provide, then move to another paradigm. So again, this is... Usually people will call it large-length model. Some people call it foundation model. The reason for that is really large. The reason for that is the foundation. And this is the foundation. It's a transformer architecture. We will not go into detail. But the interesting thing is that we can see in the middle here, the graph. The blue line here is, again, the increase of the model size. But the blue line here is the traditional model.

convolution neural network, it still increased but the exponential is not that high compared to the green line. So the green line here starts from transformer model attached to OUD in 2017. It's a paper that Google published used transformer model for translation. So interesting that a lot of people tried to solve the problem for translation but this kind of transformer model actually kind of applied to many different fields, not just for translation. And again, two most important, it's GPU friendly, it's easy to accelerate by GPU. Second thing, you don't have to label things. You don't have to go to that tedious annotation process. So because of these two reasons, boom, again magically everyone just move their data to transform based model. You can see that very large GPT-1, GPT-2, GPT-3. And the most important thing is here.

traditional NLP approach or I probably will remove NLP here, traditional machine learning or deep learning approach, you need to label the data. You definitely need a lot of annotation data. The other one, the model is huge compared to 15 years ago with my, I mean 10 years ago in the market, this is still huge but the design model capacity is very specific which means every task you need to collect a new model, a new data, annotate the data, train the model, deploy the model, every task you need to go through the same process again and again but large-length model or foundation model, I would not say no but definitely need very limited, I mean much more less data when compared to traditional approach but the key thing is that the model size is billions to trillions and it become a more general model.

So that's why we call it foundation model. If you become a foundation and do crumb tuning through crumb engineering, we can leverage this on foundation to many other tasks. You don't have to go through that kind of a tedious annotation process too much. But if we go back to the generative AI or transformer-based model, this is actually a generative process. It just generates words from what it learned before. For example, this is, yeah, I'm sorry. I'm not good at this. Okay, so the wonderful thing about tigers, so every word's just generated by how it learned from previous data and how previous words comes out. So this is naturally just a generative process, generate word by what it learned, generate word by what previous words is, or previous two, three words is.

But because of this foundation model, we can do some kind of so-called zero-shot landing, field-shot landing. We provide a little amount of data. This is so-called prompt engineering, prompt tuning. I'm going to go into detail later, but this kind of prompt engineering can leverage a foundation model to actually make this breakthrough for the second breakthrough of AI we see nowadays. Let me give you an example of prompt engineering. For example, if I give a query, what is the yellow bar in an egg? This is a query from me. And I ask ALM, I ask CGPT, any kind of, for example, Lama 2 or Falcon, any kind of downstream model. And the result comes from this ALM. This is the part that is suspended in the center of the egg.

It's not incorrect, it's correct, right? But we don't want just the LF to come out of this. So we need to provide different contexts, different prompts to make sure that the LF can answer the right question. For example, I need LF to become a nutrition. I need LF to become a predator or a culinary. Then every different context, every different scenario, we need to provide more information to LF. This is so-called context prompt engineering. So, and the other thing is that you can also train a little small model to do this kind of prompt engineering, this so-called prompt, prompt sub-prompt or P-tuning, prompt tuning. So, and this kind of prompting is revolutionized the overall AI application developments. On the above here is a traditional process. You need to collect the data, train AI.

model, deploy the model. But with prompt-based model, you just specify the prompt and deploy the model. Let me give you another example of prompt. I need to stay in Taipei in these three days, and I need to give a hint to my wife. And in this scenario, my wife is AOM. And you can think of what kind of prompt or what kind of assumption, context I provide to my wife that he can approve my trip to Taipei to stay in three days. I've got a kid seven years old. That's nine years old. So one of the prompts is that, OK, I need to go to Taipei. I need to stay three days. I cannot take care of all the other house tasks, like laundry, mopping, this kind of stuff. And I cannot bring our kids to the school. So can you take care of that, because I need to go to Taipei to give a speech during semicon. And this is a prompt I give to my wife. And you can think of, is this prompt a correct prompt?

innovative applications within manufacturing industry. So today's topic is to unleashing the creativity. So as Scott also mentioned in the beginning that generative AI with large-scale model or foundation model actually create a really huge opportunity for all the domain, all especially for smart manufacturing. And NVIDIA is a platform company. We not just keep improve our hardware, but we keep improve our software to become, to provide this kind of a serrated computing step for enterprise. And today I'm going to start from this one. So it's pretty simple, but as Jonathan introduced me and I, before joining NVIDIA I was in Foscom and I was working a lot of different manufacturing projects in Foscom. And at that time I was so lucky.

Probably not, because my wife probably will say no, because I did not give the right information. What is the right problem? The right problem is like this. B, I think the next three days there's going to be a kind of a discount in a department store for Chanel, for Gucci, this kind of purse. And then I can buy you some gift during my stay in Taipei over the three days. And never mention about any kind of a kids stuff, housekeeping stuff with my wife, then this should be a good problem. And after I put this problem, you see I successfully got here, give a speech during Semi-Con and I stayed in Taipei for three days. But because this is another online event, so my wife will not see how I share it. But this is the magic of problem. If you design a good problem.

then you can get a better result, a good result, a specific result from the AOM because AOM nowadays they have a reasoning ability but you need to leverage this kind of prompt and another way we call it in context it's very similar to how we communicate with each other we need to give a right context to better get the feedback from AOM, from your colleagues I only think, I think I have a few minutes left but let me share another thing to you so NEMO framework so Amelia actually got a NEMO framework to help customer develop your AOM either from scratch, fine tuning or we have a guardrail here so guardrail is really important when you deploy a model to production you need to make sure the response from AOM need to within the very specific boundaries for topic, for safety, for security so guardrail is a tool for us to nail down the

the output from the LLM to a specific output because in production, we don't want the LLM to get back the wrong results. So all kinds of software stack that we work with customers and finalize this kind of platform for customers to customize their LLM models, to deploy their LLM models, or even to data process their data for training the LLM. And this is one of the example, I mean, how enterprise leverage their LLM. It's not just used car or not just on-premise. It's actually a hybrid model. So on the X-axis is the data sensitivity. I think most of you should be very familiarized. You have all kinds of data. Some of data are fake data. Some of data are infection data. They are quite sensitive. But some of data from your office guys, they are not that sensitive. And on the Y-axis is the R&D cost, resource investment from you, from your enterprise.

So, as you can see, we see that factory users, office applications, definitely in your scenario, but you can leverage OpenAI from Microsoft, from Google, from AWS to quickly come out this application. But for factory users, you can still build up your on-premise platform to create your customized platform to deploy your LLM. Okay, I think I run out of time, and this is what I had today and hope that give you some idea how Apedia works in the world and how Apedia SoberStack can help and can work with you to build up this LLM applications. Thank you so much. Thank you, Dr. Liu. I hope I can give you the right prompt questions so that I can include the right answers. I think for the protocols, it's a very interesting...

Pertama sekali, saya ingin menjemput soalan daripada penonton di dalam forum ini. Ada soalan daripada penonton? Saya akan menerima soalan. Saya melihat banyak soalan di aplikasi ini. Tetapi sebelum itu, saya ingin menerima soalan daripada penonton. Terima kasih.

to join NVIDIA like seven years ago and seven years ago this is the key message that we deliver to our customers. Three most important ingredients if you want to make something innovative. The first one let me start from the right your left-hand side corner there is the algorithm that is I believe most of you should be very familiar with deep learning today. It's just a neural network but if you combine neural network or deep learning with GPU and GPU that provide a computing power and then because of most of you come from a specific domain for example manufacturing you probably have you definitely need had a lot of data so the message seven years ago we deliver to customers is that if you combine these three different ingredients the most important ingredients then you should see something

really break through in your area. GPU provides computing power, algorithm provides algorithm to learn from data, and data. So, and not just these three ingredients, we see that GPU keep enable deep learning, and GPU in some way keep accelerate the data processing. And these three ingredients is not just independent one, it's actually work really close together. And that is seven years ago. So I will start from this one, the general ingredient, and magically seven years later, today, 2023, Star Trek, language model is still the same ingredient, but I'm gonna share more details with you. So let's start from the data one. So this two graph shows the most slope. The x-axis is the timeline from 2009 to 2021. And the left hand side is the number of transistor you put in a.

The more transistor you put in the area, which means it can provide more computing power. On the right-hand side here is not the computing power this chip provides. This is actually the computing power that, for example, TSMC, UMC, all the manufacturing customers need to provide to manufacture this unit. So the idea is that the more transistor you put in, humankind can enjoy the computing power. But at the backend, all the manufacturers need to spend a lot more computing power to manufacture this chip. And you can see the growth is exponential. And I just start to put 2021, you can imagine 2022, 2023, nowadays we have foundation model. The growth is even exponential. So, when I see this graph from some...

some experts sharing, I definitely realized I was so lucky to join NVIDIA seven years ago. Because as you see, the growth of the transistor, the growth of the computing power is naturally happening. Okay, so this is, although this is from semiconductor world, but you can think of any kinds of, for example, from healthcare, they have more and more data. We have IoT, we have all kinds of sensor data to collect the information, so the data growth is keeping increasing. And that data growth is not, I mean, it's because of, for example, deep learning, because algorithms, we have a better algorithms to learn from data, it becomes a software, write software process. You don't have to rely on, for example, rule-based algorithms. You have a better algorithms like deep learning to write a software, to become an analogy, to mapping input and output and create

a lot of applications and because of as you know that NVIDIA is a parallel computing company so if data keep grows like this as an exponential one then the CPU scale is not the solution. So as you can see from the right hand side here is the computing power grows by CPU and grows by GPU in green line. So naturally GPU computing or parallel computing becomes more and more common or practiced across many industries especially in semiconductor. This is a really tremendous transform what we see from we observe from the industry. And here's another evidence again the x axis is the timeline from 20 maybe 2010 and to 2012. I'm sorry 2012-2025.

