Global Black Balance. I'm very excited to share our LG Open Air best practices, specifically for the conductive industry today with you. Allow me to set the scenes up front. So over the past few months, you've done a great job discovering the potential of large language model. You've done envisioning what it can do, what it probably cannot do, establish the use cases and design proof concept, develop MVPs, prototype, even including the tuning and optimization, trying to make it as perfect as possible. So you've done all the right works. Awesome. The natural next step would be to go live and go to production and start serving your end users. Okay. Today's topic brings the best practices that we highly recommend to share with you. Lots of these learnings come from our learnings with customers and also our own experimentation that we don't want you to miss out.

Right, let's dive into the topic. The first one being the enterprise security. I know security is off the top of mind. It's most important aspect for launching any service facing your end users. The part of the fact that Azure OpenAI is part of Cognizant Services, and this picture shows the Azure AI family. Azure OpenAI Services is part of Cognizant Services that comes into the Azure AI family underpinned by the Microsoft Azure Cloud. And what this really means is we are protected by the multi-layer security model. The Azure OpenAI sits in here, surrounded by all kinds of protection to help make the services more secure and reliable when you offer the services, including things like private endpoints, private network connections, closed virtual networks, firewalls, network security control, et cetera, et cetera.

sanity check, you will greatly reduce the risk of something inadvertently being shared to Azure services, even though we are sharing those in a highly secure environment, but not sharing is always the best. We also at Microsoft offer a built-in content filtering service called Content Safety AI, which is a fully customizable built-in layer for protecting, moderating the inputs and outputs of the information that's being sent to Azure OpenAI. If you do not have a custom layer of protection, we also recommend you to have a look at Azure Content Safety AI, a much easier way to employ additional layers of protection on the inputs and outputs that you deal with Azure OpenAI service. Make sure that when users interact with Azure OpenAI, all the inputs and outputs that users are aware of.

are actually locked, and lock is provided by default by our Azure OpenAI services. But more importantly, is to have a routine to regularly monitor, check, and analyze the contents of the lock, do not leave assets, employ third-party tools or your own custom developer tools to periodically check for traits of abuse or traits of threat, inappropriate usage, abuse, et cetera. While this process can be done using any third-party open source or your own custom tools, we highly recommend Adoptive Sentinel, which is a Microsoft proprietary cloud security monitoring software. Microsoft, interestingly, is known more as a security company than itself as a software company. We have invested so much in protecting the Microsoft Cloud and Azure, and most of these proprietary algorithms

into this protocol Sentinel, which you will be able to tap into and leverage the state of the art threat protection, threat intelligence to help analyze your user behavior in logs and also detect incidents, monitoring, responding to it and run some investigation in case incident or anomaly was discovered. So make sure you do not leave your log unmonitored, employ some sort of protection analysis mechanism to periodically run over your log to understand how your service are being used by your end users. We at Microsoft also offer two types of content filtering or content moderation mechanism. One is called a synchronous real-time content filtering, which is when the user sends some inputs to the model and model respond to the user's inputs, these two directional interaction will be monitored by AI in real time to make sure that.

We're not sending or accepting harmful contents or content that may be inappropriate. That is known as the Content-Future Asynchronous Process. We've also employed a Hindu-based asynchronous process known as the Abuse Monitoring Process, which I briefly mentioned earlier, that the data, all of your input, output, interaction data is actually stored securely on Microsoft Edge configuration stores for highly regulated and trained security experts to go in and make sure that compliance is not breached and AI is being used in an ethical way. Now, while this layer of security is good, however, it can be uncomfortable to customers operating in a highly regulated sector such as yourself. My recommendation is apply to opt out this human abuse monitoring system mechanism in place. Although there will be some requirements and.

in the approval processes of the client, but for customers operating in a regulated industry such as yourself, chances are you will be able to get approval for opting out from this human innovation, human monitoring process, which make your data secure and left only accessible to yourself. All up, we've talked quite a lot. We wanted to make sure if you're going to leave your graphical interface as your AI photo accessible to your user, we restrict that to only educated power users and make sure that you use role-based access control and multi-factor authentication. It's more ideal and recommended to have a communication interface brokering between your user and Azure Open Air Services so that the additional layer of protection, security, compliance policies in place are inherited in the process of using your Azure Open Air Service.

recommend you to have your own custom filter protection to check the inputs and outputs of the data your user is dealing with Azure OpenAI, as well as having a broker service to protect the provisional information such as key and capture extra information useful for log monitoring and also for load balancing. Make sure you have a mechanism in place to routinely monitor and check your log and analyze for potential existence of any threats. Now with all of that done, you are in a very good position to not only protecting your Azure OpenAI from outside but also from inside out. I highly recommend that you employ consideration on all these aspects. Okay, now I have a pretty good security, highly secure Azure OpenAI service. Can I go to production? Maybe not so fast. You know, they say if something can happen, chances are they will likely to happen.

Unfortunately, this also applies to the hacking activities of malicious act. Here I've listed a commonly known, very often seen, different mechanism trying to hack into your large language model and trying to abuse it, steal information from it. I'll start with some example known as the prompt leaking. This is a particular malicious act trying to steal the instruction, the metaprompt information from the large language model by bypassing the system prompt or the security layers. You will also often see prompt hacking activities which is designed to steal information from your prompt and then hijacking it or jailbreaking it so that it will be replaced by malicious instruction instead of the instruction you designed for the large language model. There are also types known as the identity impersonation or role-playing which will override the large language model to start thinking.

itself as someone else or something else and try to impersonate other beings to start responding to the hacker's question. There are also attacks known as line attacking which will again work similar to role-playing but allow your last language model to start impersonating something else by giving you the more freedom to do things that it is not designed to do, breaking your constraints at hand. It sounds pretty terrible but we've also learned the hard way there are lots of effective defensive mechanisms that we want to share with you to deal with those injection attacks. One is called sandwiching your user input between the two meta prompts. The prompts comes after the user input will typically override the additional instructions, the malicious instructions especially included in the user input so that it will not accidentally be leaked into the fun behavior of the last language model. So sandwiching the user input

in between your control metaprompt is a very effective way to protect your large language models instructions. Also, there are some other ways to generate random sequences to be used as a closure sequences. So anything that user inputs enclosed between these random sequences will be considered user input and hence you're limiting the user's input, the efficacy of the input within your randomly chosen sequence closure so it doesn't go beyond it. Also, it's a clever idea to ask for very intelligent large language model to have a sanity check on the user's input by asking, hey, do you think whatever the user is asking you to do, is that safe to do or not? Yes and no. And only pass those ones marked as yes as an additional pair of eyeballs. For those use cases that we already know the inputs or outputs or the inputs and outputs are.

is controllable, it is a good idea to explicitly define what are the allowed inputs like and what are the allowed outputs like. Also, defining a blacklist of those inputs containing keywords that should not be present in the inputs or outputs. So explicitly calling out things that the last language model can accept, can do, or do not do. Those rules will help last language models to behave, converge its behavior and make it more controllable. I've only touched a very shallow list of all the attacks and defensive mechanisms already known. In fact, no matter how hard you try to prevent an injection attack, it will never be 100%. Hence, it's more crucial to have a mechanism process in place to constantly evolve, review your defensive mechanisms and make sure that your last language model grows over time and becomes better and better at defending itself.

which is also further surrounded by the common security features such as Azure AD authorization, certificate, custom domains, key protection, etc. And it's further underpinned by industry-leading compliances and certificate and also Microsoft investment into making Azure a more secure cloud platform for offering the services. Now what these multi-layer security models mean to your specific Azure for the instance is you are very well covered from outside in. In fact, we at Microsoft employ the principle called Microsoft Cloud Security Baseline, which has a long list of key principles adhering to the higher level of security and compliances such as identity management, privileged access, network isolation, encryption, monitoring, etc. I've only listed a few examples here before I

itself from the norm of on-premise injection attacks. And it's important to have the process in place while they're trying to make everything 100%. Now we have talked about security, and we have talked about how to protect your large language model. Let's look at the stability and predictability aspects of your Azure OpenAI services. Now most of you would have been familiar with using a PAYG fashion when you interact with Azure OpenAI services. What this means is you're actually operating your model in a shared environment, and this shared pool of backend service will be deployed in your region to handle all kinds of customer requests incoming and outgoing. This is usually fine and leaves a lot of flexibility until one of your neighbors started to overuse the services deployed in the same region. We're not saying that they're doing that intentionally.

but sometimes this unpredictable workload or traffic can cause dramatic influence to the server clusters deployed in the same region that your service is hosted on. The fact that it is a multi-tenant shared environment can bring down the availability of the services that's hosting your Azure OpenAI models, and that can result in a throttling, and sometimes in the worst case, service denial. Because someone else has done something, you don't really want the behavior caused by others to start influencing the availability of your service. In order to counter that difficulties, Microsoft, starting from the end of June, Microsoft is starting to offer this concept known as provisioned throughput, which is your dedicated instance only reserved for you, for your model and your use cases, and will deliver a more controllable, predictable performance and throughput, and also we implemented that.

cost model which allows you to control your CAPEX instead of a consumption-based model, you will commit upfront and have a fixed cost, mostly on a yearly basis to have the instances dedicated and reserved for your use cases. Now, what's worth calling out is just because you sign up to provision through, it doesn't mean that becomes the only option you have. You can actually mix and match traditional PAYG fashion, have certain workloads running PAYG and other more mission-critical, mission-crucial models and workloads running in PTUs. But, it sounds great and also at the same time, some of the customers, including maybe yourself, will start saying, oh, that sounds very expensive. I'm pretty happy with PAYG because the fact that something goes down is very rare. Microsoft does everything they can to order the SLA. Why would I even care?

about looking into the PTU. The PTU is not an opportunity to give you more performance or throughputs or quota. They are all same as PLIG. However, we put particular emphasis on bringing more stability to you, more control to you. Because in the end, we know you value your user's experience. By having a PTU in place, you will be able to secure the throughput, make it more stable and more predictable to your end-users. And that's the core value of PTU, which we highly recommend you to consider for your go-live, go-production service, especially when your service is designed to face mass market. You want that added stability for your service before you roll it out to your end-users. Since we talked about the stability, I also want to touch on a very crucial piece of the history of PTU â€“ the value of the device.

Starting from 2023 June 9th, we have implemented a new system called Region-Wide Capping System. What that means is, it doesn't really matter how many models or endpoints you deploy, they will all share the same amount of pool in terms of quota allocated per region, per subscription. You can choose to deploy N numbers of GD35 turbo model or GD4 models, however many amount of endpoints you'd like to deploy within the limit, they will all share the same given allocated pool. First thing you should be very mindful of is make sure you maximize the given quota. Distribute those quotas according to your use case, your target audience, by team, by use case or even by business unit. However you choose to distribute the given quota, make sure you always maximize the quota you use. Do not leave any positions because those will be taken away.

The residuals are wasted and we want you to maximize your return of investment. Now secondly, because we have moved on to this region-wide capital system, it becomes crucial that you have a low-balance situation at global level, across region level. What this means is for all customers approved to access GPT-3, GPT-4 models, your quota allocation, the maximum quota allocation, is distributed globally. Now again, if it's possible for you to consider leveraging your global quota allocation, make sure that you have a low-balance solution in place to tap into those residual capacity from other regions and do no waste. There are actually lots of ways of doing such global cross-region low-balance. For example, what I'm sharing here, my apologies if the picture looks busy, I've tried to simplify it, but when you implement it, it's actually very simple.

quite straightforward. You are able to combine resources from different regions, protected in their own virtual network, exposed only through the private link, heavily guarded secure private link for load balancing and workload distribution. Make sure that you have this region-wide, cross-region-wide Azure OpenAI services all connected through the Microsoft global network and content delivery system so that your end user is imperceptible to where the end server pools are. They are all able to be routed to the global resources allocated to you for different subscriptions. In fact, at Microsoft, we offer so many different services for allowing you to load balance your workload. Each service is different with different mindset, pros and cons, values and disadvantages in case you are actually able to mix and match different options and solutions.

to suit your design preferences and considerations. Since we're already on the topic of load balancing, I thought it's also important to touch base on another very popular topic, which is cashback. And when you have your resources distributed globally, some may be in PTU, which is a little more expensive than the PAYG operating model. You want to make sure that these users can be aggregated not at the higher level, but at far granular lower level by teams, by individual users, by use case, and even by business units. The way you can do that is having an APM management to capture these users' information. As I already mentioned, APM management would help you to capture those telemetry data, useful for creating customer reports, customer logging, and also for you to design your own chargeback mechanism. Example here.

use this Microsoft Azure Event Hub and stream analytics to capture the user information in real time and display that onto a live dashboard. So that with maybe a couple of seconds of delay, you will be able to see how many users are actually using your service right now, who are they, which department or group they are from, and use those information to create your charge-back mechanism to distribute the cost. So, I know I mentioned a lot. I wanted to lastly give you a piece of advice. You know, AI space is developing very fast. Some of you, including myself, are fearful of whatever I do today, tomorrow it's going to be irrelevant. At the pace at which the large language models evolve so fast, I'm not sure if things that I do today is the equivalent of tomorrow.

share a piece of messages from Microsoft that we are not focusing on or prioritizing on making the model bigger, faster, and more powerful. Instead, we are more interested and heavily invested in folks like you, a developer-first, not model-first strategy. Back in Microsoft Build announcement in May, you may have heard our CEO talk about the grand vision of Azure AI Studio. If you look into all the details of making up Azure AI Studio, none of this is designed to make the model more powerful, starting to look different. It is actually more focused on you trying to make your life, your development work easier by giving you tools and support to harness the power of your eyes language model. So do not be fearful. Whatever you are doing today is likely going to be able to continue on to tomorrow with Microsoft focused on you rather than the model itself.

are making you feel boring. Now, the good part is all of these principles are tied to some kind of features offered by economy services, which is the foundation to your Azure Open Air Services. What this means is a lot of these identity management, network protection, segregation, are already provided to you as a feature out of the box. There is no custom development needed. All we have to do is turn it on, put in some information, and leave it on as is. And with that done, you will be in a pretty good shape protecting your Azure Open Air Services. Now, I know when it comes to security, pretty good is not good enough. What we have talked so far is protecting your Azure Open Air from outside in. But because of the uniqueness of large language model, there are a few vulnerabilities that can exist within the Azure Open Air.

I have a lot, but believe me, we've only scratched the surface. Actually, things that I've shared with you from the earlier session are only the tip of the iceberg. You're welcome to reach out to my DressUpTown logo team, and we will be glad to have a more deep dive, delve into the details, discussion with you, and have a bespoke session based on your business needs. I hope you enjoy this session. Thank you.

your large language model services. Now, this picture depicts a typical workflow of your internal users, such as your employees, developers, interacting with the Azure OpenAI services backed by Microsoft. Over here are the models managed by Microsoft and provided to you in order to deploy into your Azure tenant as a main point so that the end users can interact with one of these model as a service. You may choose to upload some data, such as the data required for fine-tuning a model or the data used as a reference point for the GPT model or Azure OpenAI models to reference and start providing more relevant information. You will have additional layer of protection by Microsoft periodically coming in to check the inputs and outputs in your interaction history with Azure OpenAI and those information is securely managed and stored by Microsoft for 30 days or.

and ethical AI usage is strong. Now, as you can see, there can be a lot of points in which vulnerability can exist from within the services. Let's have a look at what these are and how do we recommend you to remediate some of the vulnerabilities and risks. First of all, Microsoft Azure OpenAI offers a very powerful GUI graphical user interface known as the Azure AI Portal. While this portal is very powerful and convenient, it allows your user to interact with large language model without writing a code, and writing a code can be very tedious and boring. However, it is at the same time, it can be quite risky because once your user have access to this portal via GUI, they are just going to be able to do whatever they want to do, leveraging the large language model settings behind them. Our recommendation is to turn off the Azure AI Portal graphical user interface.

But in case if you really need to turn it on for powering your business users, at least our recommendation is to turn on the Azure AD road-based access control. Do not leave it open to everyone who is allowed to access your Azure Open Data Service. Limit those to power users who are trained, well-educated, knowing what to do versus not to do, and only leave to those educated power users for interacting with the GUI. Also, turning on the multi-factor authentication on registered and approved devices adds another layer of security for the power user to access the Azure Open AI service via GUI. Highly recommended that you do so. Another better alternative, if it's permissible, is to leverage communication tools such as Teams as a meeting or as a window to interface between your user and the backend OpenAI services.

So it is, number one, you still offer a GUI, graphical user interface for your user to interact with Azure OpenAI services without writing code. But also at the same time, allowing your user to inherit the same security and communication compliance policies enforced by your IT department. Now these are important for later on checking the compliance, making sure the user interaction with Azure OpenAI is adhering to your corporate strategy and corporate compliance. Now when it comes to developer, including myself, we developer are very powerful bunch of people. Once we were given access to the backend OpenAI services and keys credentials required to use it, we can discover anything we want, which is great, but also dangerous. Our recommendation is for you to have a broker service.

sits between the developers and your back-end services, such as API management, then one great benefit of doing that is it will enforce your user to use the Azure AD authentication and managed identity, which is a much, much more recommended and more secure way of authenticating your user against the back-end services. It also helps to reduce the risk of inadvertently losing the key or sharing the key as a result of a disclosure of important credential information. Another benefit of having this broker API management in between is it allows you to capture the inputs, the outputs, your user behavior, your user activities, in much finer, granular detail so that you can later use this information for conducting things like intelligent load balancing, which I'll cover later on, but also aid you to compliance checking, monitoring, among other things important to secure your Azure.

and AI services. Highly recommended to adopt a broker service such as API management when you offer the service to your end users, including your internal users. In a much similar way, when it is permissible, we highly recommend our customers, especially customers such as yourself, operating in a highly regulated industry, dealing with sensitive information, to employ your own custom layer for sanity checking the inputs and outputs of the data that is being or going to be sent to the Azure OpenAI services, including those data going to be uploaded to Azure for use with Azure OpenAI and large language model. The reason for that is we wanted to lower the risk of inadvertently disclosing highly classified information or information that should not be used by Azure OpenAI or large language model-based applications

