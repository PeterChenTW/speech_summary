This is my first time speaking at a study con in Taiwan, and I'm looking forward to being back again next year. And my talk is about addressing the data challenge in Semiconductor for AI and ML applications. So I think a lot of you know Synopsys as a very strong PA company, but not many people know Synopsys as a very strong area of manufacturing as well. And today, hopefully I can enlighten you on some of the approach that Synopsys has and how to address this issue in manufacturing. As many people talked earlier about the data challenges, Fred Dell and also Oliver Spitter talked about the growing challenges in sensitive data.

on the right-hand side, what I have is a chart of numbers of transistors that is actually increasing on the chip at 0.5. And second to that, to make these chips, it requires very complex processes and very complex equipment. That is, complex processes to make these chips. And to understand what is happening on the ribbon, there are many more sensors and more data being collected from the two endpoints. And on the right-hand side over here, basically you can see the numbers of fabs that are alloying here from year to year. And a lot of these fabs are integrated together to work together to make these advanced chips that we see inside of the word chips. What we have is a continuous growth of the industry. And what we require is a very sophisticated application to be able to handle all of this challenging data scaling.

people to do that. What we want is on-time delivery. And on the left-hand side, what you see is the amount of time that we need to actually create and build the models that is needed to create insight for the customers. Of course, we all know that a training model does take a long time, and sometimes if it takes too long, the insight comes too late or it's outdated. Of course, the solution to that is we can scale out the GPU hardware, but that becomes an impractical solution because the bottom line is no longer there. What we want to be able to do is to be able to do incremental learning. Learning and adapting based on the data that we collect on a day-to-day basis. But most importantly, like I mentioned, is we need to be able to reach the right users. And the right user here needs to be able to understand and use that insight that we deliver to them to make action.

And to be able to do that is we have to consider the learning curve that we will be able to discern when they are faced with a new application that is AI powered. Of course, what we can do is we can create a brand new application with brand new insight, but the learning curve can be very high. This is something very important to them. What we want to be able to do is we want to be able to embed or integrate within this standard operating procedure inside of the fabric. So the model we usually see is if you have an FAC system, let's go ahead and replace a certain part of it with the AI. Or if you have a run-to-run system or an R&D system, you want to do the same thing. The second purpose that we see AI being very, very useful is being the connecting fabric between standard operating procedure, meaning your engineer passing the information to the process engineer, to the engineer. How can we use AI to better enable that passing of information?

And we believe we've been able to do that. The German group is very, very well. Because it is the exact same standard of operation here, but it's a streamlined process with an engineer. An analogy that I like to use when we talk about this topic of synopsis is that these three conditions, it's very similar to a person navigating through a city going to a destination. When we talk about data depth, it's like the roads that you build. There are new roads that you build. There are new bridges that you build. And the data depth is being aware of those new roads and just new bridges that are built inside of your city to get to your destination faster. The insight discovery is knowing when to use those new bridges for new roads or sometimes to use them for methodology because they're better than AI.

And lastly, urgency. Urgency is perhaps the most important part is that I need to get here on time. You don't want to be there late. How can I use the information that I need to get here on time? And when we talk about this in the perspective of smart manufacturing, we're talking about the fact that we're very good at understanding everything that happens on the earth and everything that happens on the wafer. But, you know, the world is a much bigger place. There are different cities that we need to connect one another. But Synopsys is very strong in the product, the design, the mass, the T-hab. We're also very strong in setting the infield. Our roadmap is how do we actually build bridges between these different cities together in order for us to actually find an insight that can help us enable design-sensitive process or process design-sensitive.

when we're looking inside of the field, when a product is already made, how can we improve the process or how can we improve the product? Our idea here is the data continuum. The data continuum is the way for the connecting fabric that I mentioned earlier to connect these different worlds together. I can actually remember, coincidentally, we just released a press release two days ago. We announced on FabFabTA, the FABTA stands for Data Analytics. And it's a very big effort to allow us to penetrate the man's natural world with a lot of human's mind. And of course, the FabFabTA, the whole goal of it is to be able to connect all the fab data together to improve process control and to improve the fab efficiency. And the other thing that it provides as well is the background that data scaling issue.

The topic I'm going to include in this slide is the market tension to be able to scale the system to accommodate the scaling challenges today and also to beat up more than we expect in the future as well. In summary, what we see here over at Synopsys is that as the data complexity, as the process and the chips are increasing in density, there is going to be a very strong need for a better system. And what we have over here is the capability to be able to integrate all of this data and allow you to have a platform to use our AI technology, use your technology, your AI technology or other vendors' AI technology to get the insight. But more importantly is the application that is able to embed itself inside of the ecosystem at the fab in order to deliver the insight in time and emergency.

And I hope my presentation was informational for you. And thank you for your attention. Alright, so is there any questions? Oh, there's a question here. Okay. Okay. It's not a traditional EDA company. For this manufacturer segment, are you producing yourself as EDA or TELA and UNICEF?

Yes, you're correct. So, Synopsys is definitely a tradition in the EDA company. The manufacturing portion of it is definitely a data analytics, and it's a newer platform or newer focus for Synopsys to focus on the manufacturing side of things, and the HPM. Of course, EDA is at the front of the IC life, and what we want to do is focus in the middle. And to focus in the middle, the value that we see is being able to collect that information and extract insight to improve fab efficiencies. So all of that is about data analytics. Yeah, understood. The business model will be changed. Can you repeat that please? The business model will be different. So you're going to interact with the manufacturing customer more often.

Yes, it's a question whether we're going to be interacting with different customers, different engineers. That is correct, yes. Thank you. Okay, do you have another question? Okay, so let's thank the speaker. Thank you.

But before I talk about the data challenge, it's important to understand what is driving risk, what kind of problem is actually creating inside of the data for the engineers. What you see in the green boxes over here on the right hand side are all of the different silos of application. What I like to call are domains and engineering expertise. So you have the product, you have the process, and you have the engineers. And of course, all of that together leads to generating data by itself. But who is actually at the bottom creating all of this infrastructure to create it on the IT people? And on top of that, data scientists are needed to be able to actually process all of this information in order to solve the complex problem that we see today. And the problem that we see today is that in manufacturing, that all of the engineers are trying to solve on a daily basis to prevent external problems.

increase movement, and of course, continuity of movement. And all of these actions right now are taking longer and longer. And one of the reasons why people are taking longer and longer to stop this problem is the challenges of today's problem requires across the community. People need to understand and be aware of all of the different aspects of where the data is being collected, where it's stored, and how to analyze all of that data. So it requires a vast amount of skill from a single engineer. And what we want to do at Synopsys is approach this problem and then help the user, the engineers, and the fab be able to address this stuff themselves. And to decompose this problem, at Synopsys what we see is three different pillars kind of creating this complex problem. One of them is understanding the data depth.

Being able to discover the insight, and of course the insight urgency. All of these pillars need to be addressed by themselves. And they need to scale in order to solve the data-complexity problem that we see. Instead of the fact, as I mentioned earlier, there are many varieties of data from those different domains that I explained earlier, but at the bottom are the application solutions. There are many different kinds of applications, as you can see at the bottom. And the FAP is a very complex and diverse domain. Our synopsis provides many applications that are applicable for WIDER, for MES, for WIFER, for Fast Process Control, and so on and so forth. These are embedded applications that are needed on a day-to-day basis to be able to make the FAP operational. Thank you.

10 years ago, the conventional approach of the approach to those students who generally always have data latency to put them in a data warehouse. The issue with this design of a data warehouse is it requires a very large effort and it's very disruptive to the operation of the people who have a place on this data application. And lastly, as many people mentioned, I think the part of the data is very hard to get. All of these different applications are provided by themselves. More data is collected. How is this data latency created? So creating a consolidated data warehouse is sometimes good logically but sometimes very impractical because it requires a lot of effort. What we see here at CMOS is to address this challenge, we need a more agnostic way to connect all of these data together. We are going to post it up on the channel.

talking about autology databases and how to use this technology to be able to connect the data. And what we have here is that we have created this software that has been released very recently that allows us to be able to create a layer on top of all of the existing solutions inside of the fab. You can think of it as a connection fabric of all of the incumbent systems in order to provide the data as needed for the data analysis. And moving on to once we have this connection fabric connecting all of the data in the right context to the users, what we would have is a connected data coming from different location, coming in different shape and form, all cohesive to one another to enable analysis. And this is some of that clean data that a lot of the presenters talked about. But what do we do with that clean data, that data that

One traditional approach that we have developed is creating a single purpose application or point solution as we call it. And it's very good. A point solution basically incorporates everything that is needed from data processing, from the AI, the algorithm, to the user interface to allow the users to consume that value. And that can become antiquated very, very fast. And when it becomes antiquated, it is very difficult to update because it's a monolithic application. What we want to be able to do is have a way to be able to use certain components in certain applications. What you see at the bottom of the screen here is a modular application where we want to be able to use or replace certain components that have to be used.

become outdated for the reason that they actually upgraded. As you can see down here, we have these three little columns in here, data, API, and insight. On the left-hand side, for Synopsys, well-known for PDA, there's a lot of AI that we have published over the past. TSO AI is one of the things, NVIDIA, I've already mentioned some of the privacy algorithms that we have created to accelerate this information finder. What we want to be able to do is reuse that technology and be able to apply that to different applications with different kind of data that is being collected inside of the family. And what that is going to be able to enable us is a more strategic prototyping, highly reusable algorithm, and continuous improvement. The other thing that we see a lot inside of many of the actual customers that we go to

is what the customer can enhance and plug in their own algorithm inside of this platform. And what we want to do here is to enable that by using this monitoring application. Coming back to why are we competing over this, what we focus on is always the benefits to the user, the benefits to the fab. And to kind of move back to what I mentioned earlier about what is the fab we're worried about, is the excursion prevention, human maximization. That is what we're driving to. That is our main focus on why we want to use this data, why we're doing what we can in our application. And of course the fundamental thing, what we want to be able to do, is provide the actionable insight that allows the users to be actually going with their own decision. We want them to get fast. We don't want them to do that one month from now. We want to provide them with all data as a component.

